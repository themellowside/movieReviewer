\chapter{\label{ch:2-litreview}Background}

%\minitoc

\section{Review of Literature}
I have researched a number of ways in which text is processed for information extraction, as well as methods for generating prose and generating summaries.
\subsection{Markov Chain Text Generation}
A very rudimentary methodology for generating prose is using a Markov Chain Model to certain degrees in order to generate a new text out of a given corpus. This can be fairly effective in generating random prose but is heavily dependant on how the input text varies and the selection of a good parameter, and does fall apart for larger outputs of text which lose structure and coherence.\\ %cite scigen here / postmodern generator

The Markov models model text by building lists of n-words (usually 2 or 3), followed by the word that precedes them in the text. Then, choosing an arbitrary starting point the next word is chosen randomly based on the frequency of how often the n-preceding words are found in the text. As the text is modelled on a real input, the output should look like it was penned by a human at least at a glance. The text produced is at the least feasible but is very likely to fall apart upon closer inspection or when creating larger bodies of text. I find the outputs of these models at least interesting as a baseline in generating movie review text, as their implementation requires only a body of text and relatively little processing.

\subsection{Document Summation}
H. P. Luhn discusses a method for the automatic generation of a literature abstract through selecting significant sentences evaluated through word frequency distribution. This is a methodology that can potentially be applied to automatic summation of a long plot summary to create a part of a review text. While results from this are feasible, no understanding of the text is made, and text is not generated - merely sentences taken verbatim from the text. This is not an issue in the context of its use in the research, but for the intentions of creating useful text out of a larger corpus it couldn't be used on its own as a solution.\\
R. Barzilay and M. Elhadad attempt document summarisation using lexical chains (representing the source text using lexical chains), an improved methodology for generating text summary which takes in to consideration the document's structure and attempts to summarize each section, but again suffering from the same problem of not producing any new text and merely sentence chunks of the input.\\
 %(http://scholar.google.co.uk/scholar_url?url=http%3A%2F%2Facademiccommons.columbia.edu%2Fdownload%2Ffedora_content%2Fdownload%2Fac%3A160051%2FCONTENT%2Fbarzilay_elhadad_97.pdf&hl=en&sa=X&scisig=AAGBfm1-hlclQyAND4sOoh9b_i8tRHRf4A&nossl=1&oi=scholarr&ei=cqieVeqaCcaS7AaX_aSoBw&ved=0CB8QgAMoADAA)

%
The Textrank algorithm is another solution for the problem of document summation. It is graph-based and is used to rank key-words or sentences in a document in order to find the most summarative sentences or key-words. It is based on the Page-Rank algorithm used in Google searches, and builds a graph out of the text. It provides strong summarative solutions as the connective vertexes for each key-word is used to vote for the most significant key-word, meaning the words selected are very likely to be the most representative of the text.\\
While all of these methodologies for summarising documents suffer from the drawback of not understanding a document, their use could be explored for summarizing a large text such as a film's plot to extract the most relevant sentences in the hope of providing a brief synopsis to a reader.
\subsection{Part of Speech Tagging}
Part of Speech tagging is a technique for automatically assigning and identifying what part of speech a specific word is (such as adjective, adverb, noun), with features for handling word sense disambiguation (eg identifying when can is a noun or a verb). Some of these are rule-based and some of these use machine learning methodologies to identify the part of speech of these words.\\
%https://nlp.stanford.edu/~manning/papers/emnlp2000.pdf
%http://luthuli.cs.uiuc.edu/~daf/courses/Signals%20AI/Papers/HMMs/h92-1022.pdf
Eric Brill proposes a system for rule based PoS tagging, noting that most rule-based taggers have substantially higher error rates than ones that use stochastic methodologies.\\
%https://nlp.stanford.edu/~manning/papers/tagging.pdf
PoS tagging could be used within a generative system for extracting valuable adverbs or adjectives referring to elements of a movie (such as performances, actors and direction) for a more precise evaluation of sentiment or choice of word in a generative system.
\subsection{Wordnet}
%http://iaoa.org/isc2012/docs/encycloped.article.pdf

Wordnet is essentially a thesaurus in the form of a database of English language words grouped by synonymity, where each group refers to an individual concept. Each of these groups of synonyms is known as a synset and are linked to other synsets through lexical relations.
The primary relations are synonym, and antonym, but also cover the relations "hypernym" and "meronym". Hypernym meaning a word more specific than a less specific word (eg ewe as a hypernym of sheep). A meronym is a word that makes up a whole (eg leg being a meronym of table). \\

When it comes to lexical choice, this wordnet could prove invaluable in terms of understanding what words or themes occur in a text, as a frequently common hypernym could indicate a word that is usefully representative of a corpus of evaluative text.


\subsection{Sentiment Analysis}
%Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for
%Sentiment Analysis of Social Media Text. Eighth International Conference on
%Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.
There are several approaches taken in order to evaluate sentiment in texts, one of which is using a rule-based system, and another being using supervised and unsupervised machine learning.\\

VADER is a rule-based model for the sentiment analysis of text. It requires no training data, as it is rule based, and is constructed from a dictionary of words that has been selected manually by humans. It also features heuristics such as exclamation points and all-caps words increasing the intensity of the sentiment conveyed in analysis. It applies noticeably well to social media such as Twitter and other social media.\\

One area of expansion noticeable with sentiment analysis discussed in these areas is that they only tackle polarity (positive or negative) sentiment. This could be improved upon with the use of a more precise list of key terms with more precise polarity terms, and produce more interesting evaluation which for the purposes of selecting language in a more complicated prose generation system.

\subsection{Building NLG Systems}
The book 'Building Natural Language Generation Systems', by Ehud Reiter and Robert Dale explains the building of NLG systems. It proposes a 5-step process for document generation, one which I intend to follow in the building of a more complex prose generation system. The steps are:\\
Content determination: The gathering of data which will be included in the document.\\
Document structuring: Ordering the data in such a way that makes sense in regard to the format of the document - such as describing the most important members of a cast before someone in a less important role.\\
Aggregation: Combining similar sentences into larger ones to provide a better flow to the document as well as a more human-like text.\\
Lexical choice: Putting data to words - such as wording poor sentiment related to an actor as a 'bad performance'.\\
Generation of Referring Expressions: Creating expressions that call back to aforementioned subjects without using the same term repeatedly, such as 'the director' or simply 'Bloggs' in the generation of a review of a film directed by 'Joe Bloggs'.\\
Realisation: The final stage, converting all of the processes performed into a text that follows syntactic rules.\\

The primarily covers the explanation and building of NLG systems which generate texts to sound human-like from data sources that are less interpretation-based than what my project aims to deliver, but it is a useful resource regardless.

\section{Related Existing Projects}
A large amount of inspiration in terms of my methodology has come from currently existing language generation systems and projects.
\subsection{NLTK}
The Natural Language Tool-Kit is a large library for the Python programming language, which provides a large amount of functionality for processing language. It offers Part-of-Speech tagging, a working Wordnet as well as pre-made sentiment analysis algorithms - machine learning and rule-based.
This toolkit offers a lot of inspiration in terms of methodologies for developing understanding of language, although it does not seem to touch upon the generation of text. It also has access to VADER and an implementation of Wordnet, mentioned in the literature review.

\subsection{Mark V Shaney}
Mark V. Shaney was a Usenet newsgroup user whose posts were generated through forming Markov chains of other posts on the newsgroup. The posts would often fool people into believing the comments were written by a real person. It is an early example (from 1948) of people using machines to generate prose in order to see how people react. I intend to explore how Markov chains hold up when generating review prose. As my project aims to create passable human-like text this approach to gathering data on how believable my systems are is also appealing as it provides much more interesting data than a more sterile Turing-like test approach.

\subsection{Twitter Bots}
A growing trend on Twitter is the automation of services and behaviours for accounts wishing to increase their outreach and handle having incredibly large amounts of follower engagements. \\
%
Archie is a service which offers twitter automation for businesses and individuals, which implements a number of behaviours from targeting people talking about a particular market and engaging with followers and other twitter users. Many content creators wishing to gain more traction will use these automated twitter services to follow, like or message users who have tweeted about topics they have related to.\\
%
Some of these bots however offer no purpose other than amusement and fooling people who may believe that they are human. These often generate tweets in a similar way to Mark V Shaney did, with Markov chain models that generate text through choosing the next word out of a corpus of user tweets probabilistically, based on words that precede it in that corpus. For example, there are bots that generate their tweets through applying a Markov Chain model to Donald Trump's tweets, and many people have applied this to their own personal twitter accounts. 
%https://twitter.com/robodonaldtrump?lang=en


\subsection{Parody Generators}
There are several projects which exist that generate text which looks believable, but upon closer inspection is clearly nonsense.
%http://www.elsewhere.org/journal/pomo/
%http://www.elsewhere.org/journal/wp-content/uploads/2005/11/tr-cs96-264.pdf
The post-modern generator uses Recursive Transition Networks (RTNs) in order to produce text, instead of Markov models, noting that the text produced from them tend to be "choppy and incoherent". A RTN is a diagram showing how a task may be performed - essentially a directed graph with no cycles such that following the graph will take you from the start to the end of a task - in this case generating a sentence.\\
%https://pdos.csail.mit.edu/archive/scigen/
SCIgen is another similar project that generates random Computer Science research papers. These generated papers have notably been submitted to conferences suspected to have low submission standards in order to test how stringent they really are. It uses a "context-free grammar" to generate the texts, which is essentially a set of rules that describe the generation of all the possible sentences used within a generated paper. It consists of sentence templates as well as nouns, verbs, adjectives and adverbs which will be used to fill in the templates. This kind of methodology could prove useful when it comes to generating reviews, as informed generation of subjective text seems like an area which has not been explored to a great degree.
\\


\subsection{NLG Systems}
There are many examples of NLG systems which exist for a multitude of different purposes.
STANDUP is a system which creates question and answer style jokes with the purpose of developing language skills in young children and those with disabilities affecting communication. The output jokes are puns, so must be generated with an understanding of word sense, synonyms and phonetic similarity in words. It aims to fill in a surface template for the words to be filled into, and present these puns through a simple GUI.\\

%http://users.sussex.ac.uk/~christ/crs/gc/STANDUP_AAI_revised.pdf
The STOP system was a system built to create letters encouraging smokers to stop smoking based on their responses to questionnaires about their smoking habits. It would use the information that they filled in to complete the leaflet, which was posted through the doors of the smokers. It was found that there was no significant effect on quitting smoking between those sent personalised letters or those who were sent regular ones.\\
%http://homepages.abdn.ac.uk/e.reiter/pages/papers/rags99.pdf

I have found it difficult to come across any NLG systems that generate review prose or any subjective prose at all, with the majority of systems being in the domain of reporting facts or wording the results of a single search problem, such as STANDUP's wording of wordplay it has discovered or the STOP system using the data from questionnaire responses. The reporting of a subjective opinion seems to be a topic that has not been touched upon, or at least I have failed to discover anything like this in my research.
